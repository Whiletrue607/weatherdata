library(jsonlite)
library(stringr)
library(httr)
library(dplyr)
library(RSelenium)
library(wdman)
library(data.table)

################# 일단은 스포츠 부분만 가능 다른 카테고리는 url 포멧이 달라서 안될 수 도 #############

start_date = '2018-01-01'               
period = 2                              
news_path = 'C:\\Users\\Lee\\Documents\\news.csv'        
comment_path = 'C:\\Users\\Lee\\Documents\\comment.csv' 
RD_path = 'C:\\Users\\Lee\\Documents\\RD.csv' 
cnt = 1
total_news_list=list()

cmt_list <- tibble(                                                         # 댓글 수집을 위한 ㅂ
  news_ID = numeric(),
  username1 = character(),
  comment1 = character(),
  count1 = numeric(),
  username2 = character(),
  comment2 = character(),
  count2 = numeric(),
  username3 = character(),
  comment3 = character(),
  count3 = numeric()
  
)

RD_data <- tibble(                                                         # 공감 데이터를 위한 ㅂ
  news_ID = numeric(),
  like =  numeric(),
  fan = numeric(),
  sad = numeric(),
  want = numeric(),
  angry = numeric()
)

time_row <- tibble(                                                         # 수집 시간을 나타내기 위한 ㅂ
  C_time = character(),
  news_ID = numeric()
)

f= c("kbaseball",                                                         # 뉴스 배너 위치 지정(x = 1~9)
     "wbaseball",
     "kfootball",
     "wfootball",
     "general",
     "basketball",
     "volleyball",
     "golf",
     "esports") 

for( i in 1:period) {                                                         # i는 기간, z는 위치 지정, a는 페이지 위치
    for(z in 1:9) {
      date = as.Date(start_date)+i-1
      date = gsub('-','',date)
      sub_url_1 = 'https://sports.news.naver.com/'
      sub_url_2 = '/news/index.nhn?'
      sub_url_3 = 'page='
      sub_url_4 = '&date='
      aurl = paste0(sub_url_1,f[z],sub_url_2,sub_url_4, date)
      ajson = str_match(GET(aurl),regex('newsListModel: (\\{.*\\})',multiline=T))[2]
      page = fromJSON(ajson)
      page = page$totalPages                                            # page 수 찾는 작업
      
      for(a in 1:page){
      

      url = paste0(sub_url_1,f[z],sub_url_2,sub_url_3,a,sub_url_4, date)                                       
      
      
      json = str_match(GET(url),regex('newsListModel: (\\{.*\\})',multiline=T))[2]
      result = fromJSON(json)
      
      
      result[[1]]$title <- gsub(",", ".", result[[1]]$title) # csv로 저장시 column구분을 위한 character 교체
      result[[1]]$title <- gsub("\"", "'", result[[1]]$title)
      
      
      tmp_news_list = result[[1]] %>% tbl_df() %>% select(datetime,title)
      
      total_news_list <- rbind(total_news_list, tmp_news_list)
      
      news_id <- result[[1]] %>% select(oid,aid)                  #각 뉴스기사 페이지를 구분하는 id의 list 
      len <- length(row(tmp_news_list))/length(tmp_news_list)     #크롤링한 뉴스기사 갯수
      
      #각 뉴스기사에 따른 댓글 크롤링
      for(j in 1:len) {
        oid <- news_id[j,1]
        aid <- news_id[j,2]
        news_url <- 'https://sports.news.naver.com/afc/news/read.nhn?'
        news_url <- paste0(news_url, "oid=", oid, "&aid=", aid)
        
        tmp_url1 = 'https://apis.naver.com/commentBox/cbox/web_naver_list_jsonp.json?ticket=sports&templateId=&pool=cbox2&lang=ko&country=KR&objectId='
        tmp_url3 = '&categoryId=&pageSize=10&indexSize=10&groupId=&listType=OBJECT&pageType=more&page=1&initialize=true&userType=&useAltSort=true&replyPageSize=20&moveTo=&sort=sort'
        tmp_url2 = paste0('news',oid,'%2C',aid)
        
        req_url = paste0(tmp_url1, tmp_url2, tmp_url3) 
        req_ref = news_url
        
        #댓글 요청 / 요청 승인을 위해 Referer로 뉴스 url를 헤더에 추가
        ret <- GET(req_url, add_headers(Referer = req_ref))
        
        #json 포멧으로 바꾸기 위한 작업
        txt_data = content(ret, "text")
        txt_data <- gsub("(;|\n|_callback)", "", txt_data)
        txt_data <- gsub("\\(", "[", txt_data)
        txt_data <- gsub("\\)", "]", txt_data)
        
        #댓글 정리
        json_data <- fromJSON(txt_data)$result$commentList[[1]]
        cmt_list[cnt,"news_ID"] = cnt
        if (length(json_data) !=0){
          tmp_cmt_list <- json_data %>% tbl_df() %>% select(userName, contents,sympathyCount)
          cmt_list[cnt,] <-c(cnt,tmp_cmt_list[1,1:3],tmp_cmt_list[2,1:3],tmp_cmt_list[3,1:3])
          }
        
          
          
      #각 뉴스의 공감수 크롤링
          RD_url1 <- "https://sports.like.naver.com/v1/search/contents?suppress_response_codes=true&q=SPORTS%5Bne_"
          RD_url2 <- paste0(oid,"_",aid,"%5D%7CSPORTS_MAIN%5Bne_",oid,"_",aid,"%5D&isDuplication=false")                        # 공감수의 위치 url
          
          
          RD_url <-paste0(RD_url1,RD_url2)
          
          #json 포맷으로 바꾸기 위한 작업
          reaction_data <- content(GET(RD_url), "text",encoding = "utf-8")
          reaction_data <- gsub("(;|\n|_callback)", "", reaction_data)
          reaction_data <- gsub("(;|\n|_callback=//)", "", reaction_data)
          reaction_data <- gsub("\\(", "[", reaction_data)
          reaction_data <- gsub("\\)", "]", reaction_data)
          
          json_reaction_data <- fromJSON(reaction_data)
          
          #공감수 정리
          
          RD <- json_reaction_data$contents[[5]][[1]]
          RD_data[cnt,] = 0
          RD_data[cnt,"news_ID"] = cnt
          if (length(RD) !=0){
            RD_count <- RD %>% select(reactionType,count)
            if(any(RD_count$reactionType=='like')) {RD_data[cnt,'like'] <- RD_count[RD_count$reactionType=='like',]$count}
            if(any(RD_count$reactionType=='fan')) {RD_data[cnt,'fan'] <- RD_count[RD_count$reactionType=='fan',]$count}
            if(any(RD_count$reactionType=='sad')) {RD_data[cnt,'sad'] <- RD_count[RD_count$reactionType=='sad',]$count}
            if(any(RD_count$reactionType=='want')) {RD_data[cnt,'want'] <- RD_count[RD_count$reactionType=='want',]$count}
            if(any(RD_count$reactionType=='angry')) {RD_data[cnt,'angry'] <- RD_count[RD_count$reactionType=='angry',]$count}
            
            }

        
        time_row[cnt,"news_ID"] = cnt
        cnt = cnt+1
        
        

        
      }
    }}}
total_news_list <- cbind( "news_ID" = rownames(total_news_list), total_news_list)    #각 뉴스에 고유id부여

total_news_list$news_ID <- as.numeric(as.character(total_news_list$news_ID))
total_news_list <- left_join(left_join(total_news_list,RD_data,by = "news_ID"),cmt_list, by = "news_ID")

#수집시간 추가

time <- as.character(Sys.time())
time_row[1,1] <-  time

total_news_list=left_join(time_row,total_news_list,by="news_ID")
write.csv(total_news_list,'news_data.csv',na="")
